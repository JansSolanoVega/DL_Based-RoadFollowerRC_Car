{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"e9RoV4ZViOcl"},"outputs":[],"source":["import segmentation_models_pytorch as smp\n","import torch\n","from torch.utils.tensorboard import SummaryWriter\n","writer = SummaryWriter('runs/unetMobileNetTerrenator_0.001Grafica')\n","device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else b\"cpu\"#mps stands for matrix product states, basically enables high-performance training\n","print(\"Using: \", device)"]},{"cell_type":"markdown","metadata":{"id":"nROC8iV5iOcp"},"source":["**Creating model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IPJmwhyFiOcs"},"outputs":[],"source":["model = smp.Unet(encoder_name=\"mobilenet_v2\", encoder_weights=None, classes=2, activation='softmax')\n","#model.classification_head = torch.nn.Conv2d(512, 2, kernel_size=1, stride=1)\n","model.to(device)\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iZSspsToiOcu"},"outputs":[],"source":["for count,child in enumerate(model.children()):\n","    print(\" Child \", count , \"is -\")\n","    print(child)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UYtALS-RiOcv"},"outputs":[],"source":["'''#First 3 layers are freezed\n","for parents in model.children():\n","   \n","   for count,child in enumerate(parents.children()):\n","      \n","      if count==3:\n","         break\n","      for param in child.parameters():\n","         param.requires_grad=False\n","      print(\"Child \",count,\" is frozen now\")\n","      print(child)'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0OYFkXeDiOcw"},"outputs":[],"source":["from segmentation_models_pytorch.encoders import get_preprocessing_fn\n","\n","preprocess_input = get_preprocessing_fn('mobilenet_v2', pretrained=\"imagenet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wed7WzE5iOcx"},"outputs":[],"source":["from PIL import Image\n","import numpy as np\n","import matplotlib.pyplot as plt\n","img = Image.open(\"Img2.png\")\n","img = preprocess_input(np.array(img))\n","plt.imshow(img)"]},{"cell_type":"markdown","metadata":{"id":"m0X-q6tyiOcy"},"source":["**DATASET**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wHo-u_z3iOcz"},"outputs":[],"source":["import torch\n","import torchvision.transforms as transforms\n","from torchvision.transforms import Lambda\n","import torchvision.datasets as datasets\n","from PIL import Image\n","import os\n","import pandas as pd\n","import numpy as np\n","print(torch.__version__)"]},{"cell_type":"markdown","metadata":{"id":"I-HZ6xJCiOc2"},"source":["*****"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGlq_coOiOc3"},"outputs":[],"source":["folder1 = \"SemanticSegmentationFotos/all/masks\"\n","folder2 = \"SemanticSegmentationFotos/all/images\"\n","\n","\n","if not os.path.exists(folder2):\n","    os.makedirs(folder2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ezijAqpwiOc4"},"outputs":[],"source":["dir1 = \"SemanticSegmentationFotos/AntihorarioMasks\"\n","dir2 = \"SemanticSegmentationFotos/fotosCarrilAntihorario\"\n","\n","for root,dirs,files in os.walk(dir1):\n","    for file in files:\n","        for root2,dirs2,files2 in os.walk(dir2):\n","            if file in files2:\n","                rutaMask = folder1 +\"/\"+file\n","                rutaImagen = folder2 +\"/\"+file\n","                imgMask = Image.open(dir1+\"/\"+file)\n","                imgMask.save(rutaMask)\n","                img = Image.open(dir2+\"/\"+file)\n","                img.save(rutaImagen)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_dkMeTEiOc5"},"outputs":[],"source":["dir1 = \"SemanticSegmentationFotos/Antihorario2Masks\"\n","dir2 = \"SemanticSegmentationFotos/fotosCarrilAntihorario2\"\n","\n","for root,dirs,files in os.walk(dir1):\n","    for file in files:\n","        for root2,dirs2,files2 in os.walk(dir2):\n","            if file in files2:\n","                rutaMask = folder1 +\"/\"+file.split(\".\")[0]+\"v2.png\"\n","                rutaImagen = folder2 +\"/\"+file.split(\".\")[0]+\"v2.png\"\n","                imgMask = Image.open(dir1+\"/\"+file)\n","                imgMask.save(rutaMask)\n","                img = Image.open(dir2+\"/\"+file)\n","                img.save(rutaImagen)"]},{"cell_type":"markdown","metadata":{"id":"PCYzfeNuiOc6"},"source":["****"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NMo0akeDiOc6"},"outputs":[],"source":["import glob\n","folder_data = glob.glob(\"SemanticSegmentationFotos/all/images/*.png\")\n","folder_mask = glob.glob(\"SemanticSegmentationFotos/all/masks/*.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qulun7qHiOc7"},"outputs":[],"source":["len_data = len(folder_data)\n","print(len_data)\n","train_size = 0.8"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mSqqabzriOc8"},"outputs":[],"source":["train_image_paths = folder_data[:int(len_data*train_size)]\n","test_image_paths = folder_data[int(len_data*train_size):]\n","\n","train_mask_paths = folder_mask[:int(len_data*train_size)]\n","test_mask_paths = folder_mask[int(len_data*train_size):]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MHY52UlHiOc9"},"outputs":[],"source":["from torch.utils.data.dataset import Dataset\n","class UnetDataset(Dataset):\n","    def __init__(self, image_paths, target_paths, transform1 = None, transform2 = None, train=True):   # initial logic \n","        self.image_paths = image_paths\n","        self.target_paths = target_paths\n","        self.transform1 = transform1\n","        self.transform2 = transform2\n","\n","    def __getitem__(self, index):\n","\n","        image = Image.open(self.image_paths[index])\n","        #image.show()\n","        mask = Image.open(self.target_paths[index])\n","        #mask.show()\n","        if self.transform1:\n","            t_image = self.transform1(image)\n","        if self.transform2:\n","            t_mask = self.transform2(mask)\n","        return t_image, t_mask\n","\n","    def __len__(self):  # return count of sample we have\n","\n","        return len(self.image_paths)"]},{"cell_type":"markdown","metadata":{"id":"tLalNqlziOc-"},"source":["**Dataloader**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_8rWGo9iOc_"},"outputs":[],"source":["import cv2\n","class Preprocessing1:\n","    def __init__(self, porcentaje_cropped):\n","        self.porcentaje = porcentaje_cropped\n","\n","    def __call__(self, x):\n","        img_array = np.asarray(x)\n","        img_array = preprocess_input(np.array(img_array))\n","        W, H, C = img_array.shape\n","        img = img_array[int(self.porcentaje*W):,:,:]\n","        img = cv2.resize(img, (256, 256))\n","        return img\n","class Preprocessing2:\n","    def __init__(self, porcentaje_cropped):\n","        self.porcentaje = porcentaje_cropped\n","\n","    def __call__(self, x):\n","        img_array = np.asarray(x)\n","        img_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n","        \n","        W, H= img_array.shape\n","        img = img_array[int(self.porcentaje*W):,:]\n","        img = cv2.resize(img, (256, 256))\n","        return img\n","class ToFloat32:\n","    def __call__(self, x):\n","        return x.to(torch.float32)\n","transform1 = transforms.Compose([\n","    # you can add other transformations in this list\n","    Preprocessing1(porcentaje_cropped = 0.5),\n","    transforms.ToTensor(),\n","    ToFloat32(),\n","])\n","transform2 = transforms.Compose([\n","    # you can add other transformations in this list\n","    Preprocessing2(porcentaje_cropped = 0.5),\n","    transforms.ToTensor(),\n","    ToFloat32(),\n","])\n","train_dataset = UnetDataset(train_image_paths, train_mask_paths, transform1, transform2, train=True)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n","\n","test_dataset = UnetDataset(test_image_paths, test_mask_paths, transform1, transform2, train=False)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"EEkqwKPziOc_"},"source":["**Visualizing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kzEc2Q2AiOc_"},"outputs":[],"source":["fig=plt.figure(0, (10,6))\n","for x , y in train_loader:#Each element x and y (from iterable \"test_dataloader\") is a batch of 'batch_size' features and labels\n","    print(x.shape, y.shape)\n","    for i in range(8):\n","        fig.add_subplot(4,2,i+1)\n","        #plt.imshow(np.transpose(x[i], (1, 2, 0)))#COnvertir al formato de matplotlib\n","        plt.imshow(np.transpose(y[i], (1, 2, 0)))#COnvertir al formato de matplotlib\n","        #plt.title(\"Direccion: \"+str(int(y[i])))\n","    print(x[0].dtype)\n","    print(\"Shape of x [N, C, H, W]: \",x.shape)#tiene dimensiones 32(batch_size)xC(n_channels)xH(Height)xW(Width)\n","    print(\"Shape of y: \",y.shape)#tiene dimensiones 32(batch_size), pues para cada elemento del batch hay un label\n","    break"]},{"cell_type":"markdown","metadata":{"id":"DMP-4cjjiOdA"},"source":["Optim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k3vq-miGiOdA"},"outputs":[],"source":["loss_fn = torch.nn.CrossEntropyLoss()#Hace el softmax y la funcion costo al mismo tiempo\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"markdown","metadata":{"id":"_tG0LjP8iOdB"},"source":["Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O9nXvPUgiOdB"},"outputs":[],"source":["def train(dataloader, model, loss_function, optimizer, epoch):\n","    train_correct_num, train_total, train_cost_acum = 0, 0, 0.0\n","    size = len(dataloader.dataset)\n","    model.train()\n","    for batch, (x,y) in enumerate(dataloader):\n","        x = x.to(device=device, dtype=torch.float32)\n","        y = y.to(device=device, dtype=torch.long).squeeze(1)\n","        print(x.shape)\n","        scores = model(x)\n","        cost = loss_function(input=scores, target=y)\n","        #cost.requires_grad = True\n","\n","        #AÃ±adir a la grafica\n","        writer.add_scalar(\"Loss/train\", cost, epoch)\n","\n","        #Backpropagation\n","        optimizer.zero_grad()\n","        cost.backward()\n","        optimizer.step()\n","\n","        train_predictions = torch.argmax(scores, dim=1)\n","        train_correct_num += (train_predictions == y).sum()\n","        train_total += torch.numel(train_predictions)#total numero de elementos de train_predictions, if size=[a,b,c ] returns a*b*c\n","        train_cost_acum += cost\n","        \n","        if batch % 9 == 1:\n","            train_acc = float(train_correct_num)/train_total#el train total ya considera el batch\n","            train_cost_every = float(train_cost_acum)/batch#Se divide entre el batch, xq por cada batch se aumenta el loss\n","            writer.add_scalar('training loss', train_cost_every, epoch*size+batch)\n","            writer.add_scalar('training accuracy', train_acc, epoch*size+batch)\n","            print(\"loss: \", train_cost_every, \"Accuracy: \", train_acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oI1Aq-PGiOdC"},"outputs":[],"source":["def test(dataloader, model, loss_function,t):\n","    num_batches = len(dataloader)\n","    model.eval()\n","    test_loss, train_correct_num, train_total = 0.0, 0, 0\n","    with torch.no_grad():\n","        for x, y in dataloader:\n","            x = x.to(device=device, dtype=torch.float32)\n","            y = y.to(device=device, dtype=torch.long).squeeze(1)\n","            pred = model(x)\n","            loss = loss_function(input=pred, target=y)\n","            \n","            test_loss += loss.item()\n","            train_predictions = torch.argmax(pred, dim=1)\n","            train_correct_num += (train_predictions == y).sum()\n","            train_total += torch.numel(train_predictions)\n","\n","    test_loss /= num_batches\n","    train_correct_num = train_correct_num/train_total\n","    writer.add_scalar('test accuracy', train_correct_num*100, t)\n","    print(f\"Test Error: \\n Accuracy: {(100*train_correct_num):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZXTy9dliOdD"},"outputs":[],"source":["epochs = 25\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train(train_loader, model, loss_fn, optimizer, t)\n","    test(test_loader, model, loss_fn, t)\n","writer.flush()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fpd6OivgiOdD"},"outputs":[],"source":["torch.save(model.state_dict(), \"unetMobilenetv2WithoutDataAug.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_csHJitriOdD"},"outputs":[],"source":["modelCharged = smp.Unet(encoder_name=\"mobilenet_v2\", encoder_weights=None, classes=2, activation='softmax')\n","modelCharged.load_state_dict(torch.load(\"unetMobilenetv2WithoutDataAug.pth\"))\n","modelCharged.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9aw1A-BpiOdE"},"outputs":[],"source":["modelCharged = torch.load(\"AlvaroUnetv1.pth\")\n","ONNX_FILE_PATH = 'AlvaroUnetv1.onnx'\n","input = torch.rand(8, 3, 256, 256)\n","torch.onnx.export(modelCharged, input, ONNX_FILE_PATH, input_names=['input'],\n","                  output_names=['output'], export_params=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ZEMz6v-iOdF"},"outputs":[],"source":["import onnx\n","onnx_model = onnx.load(ONNX_FILE_PATH)\n","onnx.checker.check_model(onnx_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Zbb9GWNiOdF"},"outputs":[],"source":["torch.save(modelCharged, \"unetMobilenetv2WithoutDataAugEntire.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdEjsOB0iOdG"},"outputs":[],"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E7eLwrZTiOdG"},"outputs":[],"source":["import time\n","model.eval()\n","fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n","\n","img = transform1(Image.open(\"Img509.png\"))\n","ax[0].imshow(img.permute(1, 2, 0))\n","ax[0].set_title('Figura 1')\n","img = img.to(\"cuda:0\")\n","#plt.imshow(  img.permute(1, 2, 0)  )\n","#print(img.shape)\n","#Image.fromarray(img.numpy()).show()\n","model.to(\"cuda:0\")\n","with torch.no_grad():\n","    a=time.time()\n","    output = model(img.unsqueeze(0))\n","    output = output.to(\"cpu\")\n","    maskOut =torch.argmax(output,1).numpy()\n","    print(time.time()-a)\n","    ax[1].imshow(maskOut[0])\n","    ax[1].set_title('Figura 2')\n","    print(maskOut.shape)\n","    a=maskOut.astype('uint8')*255\n","    print(a.shape)\n","    #imagen = Image.fromarray(a[0])\n","    #imagen.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v6YcPcXKiOdH"},"outputs":[],"source":["model.to(\"cuda:0\")\n","torch.save(model, \"unetEntireModel.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cEwJU9vNiOdH"},"outputs":[],"source":["torch.save(model.state_dict(), \"unetModelParte.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ogr-Ti_1iOdI"},"outputs":[],"source":["import time\n","modeloCargado = torch.load(\"unetEntireModel.pth\")\n","img = transform1(Image.open(\"Img509.png\"))\n","print(img.is_cuda)\n","fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n","ax[0].imshow(img.permute(1, 2, 0))\n","ax[0].set_title('Figura 1')\n","img = img.to(\"cuda:0\")\n","print(img.is_cuda)\n","#plt.imshow(  img.permute(1, 2, 0)  )\n","#print(img.shape)\n","#Image.fromarray(img.numpy()).show()\n","modeloCargado.to(\"cuda:0\")\n","with torch.no_grad():\n","    a=time.time()\n","    output = modeloCargado(img.unsqueeze(0))\n","    print(time.time()-a)\n","    a = output[0].to(\"cpu\")\n","    maskOut =torch.argmax(a,1).numpy()\n","    ax[1].imshow(maskOut[0])\n","    ax[1].set_title('Figura 2')\n","    print(maskOut.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nDWCODC7iOdI"},"outputs":[],"source":["modeloCargado = torch.load(\"unetEntireModel.pth\")\n","img = transform1(Image.open(\"Img25.png\"))\n","print(img.is_cuda)\n","fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n","ax[0].imshow(img.permute(1, 2, 0))\n","ax[0].set_title('Figura 1')\n","img = img.to(\"cpu\")\n","print(img.is_cuda)\n","#plt.imshow(  img.permute(1, 2, 0)  )\n","#print(img.shape)\n","#Image.fromarray(img.numpy()).show()\n","modeloCargado.to(\"cpu\")\n","with torch.no_grad():\n","    a=time.time()\n","    output = modeloCargado(img.unsqueeze(0))\n","    print(time.time()-a)\n","    a = output[0].to(\"cpu\")\n","    maskOut =torch.argmax(a,1).numpy()\n","    ax[1].imshow(maskOut[0])\n","    ax[1].set_title('Figura 2')\n","    print(maskOut.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUDN3SmiiOdJ"},"outputs":[],"source":["import time\n","import segmentation_models_pytorch as smp\n","modeloCargado = smp.Unet(encoder_name=\"resnet34\", classes=2, activation='softmax')\n","modeloCargado.classification_head = torch.nn.Conv2d(512, 2, kernel_size=1, stride=1)\n","modeloCargado.load_state_dict(torch.load(\"unetModelParte.pth\"))\n","model.eval()\n","img = transform1(Image.open(\"Img147.png\"))\n","print(img.is_cuda)\n","fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n","ax[0].imshow(img.permute(1, 2, 0))\n","ax[0].set_title('Figura 1')\n","img = img.to(\"cuda:0\")\n","print(img.is_cuda)\n","#plt.imshow(  img.permute(1, 2, 0)  )\n","#print(img.shape)\n","#Image.fromarray(img.numpy()).show()\n","modeloCargado.to(\"cuda:0\")\n","with torch.no_grad():\n","    a=time.time()\n","    output = modeloCargado(img.unsqueeze(0))\n","    print(time.time()-a)\n","    a = output[0].to(\"cpu\")\n","    maskOut =torch.argmax(a,1).numpy()\n","    ax[1].imshow(maskOut[0])\n","    ax[1].set_title('Figura 2')\n","    print(maskOut.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w5hxxxvYiOdJ"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"3f504c358125acf7ee3ad46607425e489c7569b21d1582a396be808f5c87052a"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}